# ğŸ§™â€â™‚ï¸ **GPT for Harry Potter: Generating Magic with Transformers**  

**Skills:** Natural Language Processing (NLP), Transformers, GPT, Deep Learning  

---

## ğŸš€ **Project Overview**  
What if we could **generate new Harry Potter stories** using AI? This project explores the power of **transformers and autoregressive models** to generate text in the **style of J.K. Rowling's Harry Potter series**.  

Using **GPT-style models**, this project demonstrates:  
âœ… **Language modeling using character-level and word-level embeddings**  
âœ… **Building a transformer-based text generator from scratch**  
âœ… **Fine-tuning models to capture the writing style of Harry Potter**  
âœ… **Experimenting with different architectures, including the Bigram model**  

This project is perfect for **machine learning and NLP roles**, showcasing deep learning expertise in **language modeling, sequence generation, and transformer-based architectures**.  

---

## ğŸ¯ **Key Objectives**  
âœ” **Understand and implement GPT-style text generation**  
âœ” **Experiment with different transformer-based architectures**  
âœ” **Analyze and improve model performance with training techniques**  
âœ” **Generate coherent, Harry Potter-style text using AI**  

---

## ğŸ“– **Dataset & Preprocessing**  
This project uses **text from Harry Potter books** as training data. The text is tokenized into sequences and fed into a model that predicts the next word (or character).  

ğŸ”¹ **Tokenization** â€“ Converting text into a numerical format  
ğŸ”¹ **Vocabulary Creation** â€“ Learning word/character embeddings  
ğŸ”¹ **Contextual Sequences** â€“ Training the model on short sequences  

âœ… **Example: Tokenizing Text for Training**  
```python
import torch
from transformers import GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

text = "Harry Potter and the Sorcerer's Stone"
tokens = tokenizer.encode(text, return_tensors="pt")

print(tokens)
```
ğŸ’¡ **Why GPT-2?**  
- **Pretrained on large-scale data** â†’ Faster training, better context retention  
- **Autoregressive** â†’ Predicts text step-by-step like a real writer  

---

## ğŸ”¥ **Building the Model: GPT for Harry Potter**  
This project explores multiple **text generation models**, including:  

| **Model** | **Description** |
|-----------|----------------|
| **Bigram Model** | Predicts next word based on the last one (simple baseline) |
| **GPT-Style Transformer** | Uses attention mechanisms for long-range text coherence |
| **Fine-Tuned GPT-2** | Trained on Harry Potter text for better style emulation |

âœ… **Example: Implementing a Simple Bigram Model**  
```python
import torch.nn as nn

class BigramModel(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, vocab_size)

    def forward(self, x):
        return self.embedding(x)

model = BigramModel(vocab_size=5000)
```
ğŸ’¡ **Key Learning:** **The Bigram Model produces random, incoherent text** because it lacks long-term memory.  

---

## ğŸ­ **Experimenting with Transformer-Based Models**  
ğŸ”¹ **Self-Attention Mechanisms** â€“ Helps retain long-range dependencies  
ğŸ”¹ **Positional Encodings** â€“ Adds order-awareness to sequences  
ğŸ”¹ **Training with GPU Acceleration** â€“ Uses `CUDA` for faster processing  

âœ… **Example: Training a Transformer Model**  
```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

model = GPT2LMHeadModel.from_pretrained("gpt2")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

input_text = "Hogwarts is a place where"
input_ids = tokenizer.encode(input_text, return_tensors="pt")

output = model.generate(input_ids, max_length=50)
print(tokenizer.decode(output[0], skip_special_tokens=True))
```
ğŸ’¡ **Why Transformers?**  
âœ” **Handles long-range dependencies**  
âœ” **Can be fine-tuned for domain-specific text (e.g., Harry Potter books)**  

---

## ğŸ“Š **Model Evaluation & Results**  
### **Challenges & Key Learnings:**  
âœ” **Bigram model generates nonsense** because it lacks context  
âœ” **Vanilla GPT-2 generates readable but generic text**  
âœ” **Fine-tuned GPT-2 starts capturing the Harry Potter style**  

âœ… **Example: Generating a Harry Potter-style Story**  
```python
input_text = "Harry walked into the Forbidden Forest and"
input_ids = tokenizer.encode(input_text, return_tensors="pt")

output = model.generate(input_ids, max_length=100)
print(tokenizer.decode(output[0], skip_special_tokens=True))
```
ğŸ’¡ **Expected Output:**  
"Harry walked into the Forbidden Forest and saw a strange glow ahead. He stepped forward cautiously, his wand raised. Suddenly, a voice whispered from the darkness, 'You shouldn't be here, Potter.' Harry turned, his heart pounding..."  

---

## ğŸ”® **Future Enhancements**  
ğŸ”¹ **Train a fine-tuned GPT-2 on all Harry Potter books** for better coherence  
ğŸ”¹ **Use Reinforcement Learning (RLHF) to improve output quality**  
ğŸ”¹ **Experiment with GPT-3 or LLaMA for more advanced generation**  
ğŸ”¹ **Create an interactive chatbot where users can roleplay as wizards**  

---

## ğŸ¯ **Why This Project Stands Out for NLP & ML Roles**  
âœ” **Demonstrates deep learning & NLP expertise**  
âœ” **Applies cutting-edge transformer models for text generation**  
âœ” **Showcases hands-on model fine-tuning and evaluation**  
âœ” **Highlights AIâ€™s creative potential for storytelling & entertainment**  

---

## ğŸ›  **How to Run This Project**  
1ï¸âƒ£ Clone the repo:  
   ```bash
   git clone https://github.com/shrunalisalian/gpt-harry-potter.git
   ```
2ï¸âƒ£ Install dependencies:  
   ```bash
   pip install -r requirements.txt
   ```
3ï¸âƒ£ Run the Jupyter Notebook:  
   ```bash
   jupyter notebook HarryPotter.ipynb
   ```

---

## ğŸ“Œ **Connect with Me**  
- **LinkedIn:** [Shrunali Salian](https://www.linkedin.com/in/shrunali-salian/)  
- **Portfolio:** [Your Portfolio Link](#)  
- **Email:** [Your Email](#)  

---

References: https://www.youtube.com/watch?v=kCc8FmEb1nY&t=3344s
